{"name":"www::robotrules","description":"WWW::RobotRules\n- database of robots.txt-derived permissions","body":"\n\n<h1 align=\"center\">WWW::RobotRules</h1>\n\n\n\n\n\n\n\n\n\n<hr>\n\n\n<h2>NAME\n<a name=\"NAME\"></a>\n</h2>\n\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">WWW::RobotRules\n- database of robots.txt-derived permissions</p>\n\n<h2>SYNOPSIS\n<a name=\"SYNOPSIS\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">use\nWWW::RobotRules; <br>\nmy $rules = WWW::RobotRules-&gt;new('MOMspider/1.0'); <br>\nuse LWP::Simple qw(get); <br>\n{ <br>\nmy $url = &quot;http://some.place/robots.txt&quot;; <br>\nmy $robots_txt = get $url; <br>\n$rules-&gt;parse($url, $robots_txt) if defined $robots_txt;\n<br>\n} <br>\n{ <br>\nmy $url = &quot;http://some.other.place/robots.txt&quot;;\n<br>\nmy $robots_txt = get $url; <br>\n$rules-&gt;parse($url, $robots_txt) if defined $robots_txt;\n<br>\n} <br>\n# Now we can check if a URL is valid for those servers <br>\n# whose &quot;robots.txt&quot; files we've gotten and\nparsed: <br>\nif($rules-&gt;allowed($url)) { <br>\n$c = get $url; <br>\n... <br>\n}</p>\n\n<h2>DESCRIPTION\n<a name=\"DESCRIPTION\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">This module\nparses <i>/robots.txt</i> files as specified in &quot;A\nStandard for Robot Exclusion&quot;, at\n&lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters\ncan use the <i>/robots.txt</i> file to forbid conforming\nrobots from accessing parts of their web site.</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">The parsed\nfiles are kept in a WWW::RobotRules object, and this object\nprovides methods to check if access to a given\n<small>URL</small> is prohibited. The same WWW::RobotRules\nobject can be used for one or more parsed <i>/robots.txt</i>\nfiles on any number of hosts.</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">The following\nmethods are provided: <br>\n$rules = WWW::RobotRules-&gt;new($robot_name)</p>\n\n<p style=\"margin-left:17%;\">This is the constructor for\nWWW::RobotRules objects. The first argument given to\n<i>new()</i> is the name of the robot.</p>\n\n\n<p style=\"margin-left:11%;\">$rules-&gt;parse($robot_txt_url,\n$content, $fresh_until)</p>\n\n<p style=\"margin-left:17%;\">The <i>parse()</i> method takes\nas arguments the <small>URL</small> that was used to\nretrieve the <i>/robots.txt</i> file, and the contents of\nthe file.</p>\n\n<p style=\"margin-left:11%;\">$rules-&gt;allowed($uri)</p>\n\n<p style=\"margin-left:17%;\">Returns <small>TRUE</small> if\nthis robot is allowed to retrieve this\n<small>URL.</small></p>\n\n<p style=\"margin-left:11%;\">$rules-&gt;agent([$name])</p>\n\n<p style=\"margin-left:17%;\">Get/set the agent name.\n<small>NOTE:</small> Changing the agent name will clear the\nrobots.txt rules and expire times out of the cache.</p>\n\n<h2>ROBOTS.TXT\n<a name=\"ROBOTS.TXT\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">The format and\nsemantics of the &quot;/robots.txt&quot; file are as follows\n(this is an edited abstract of\n&lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">The file\nconsists of one or more records separated by one or more\nblank lines. Each record contains lines of the form</p>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">&lt;field-name&gt;:\n&lt;value&gt;</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">The field name\nis case insensitive. Text after the &rsquo;#&rsquo;\ncharacter on a line is ignored during parsing. This is used\nfor comments. The following &lt;field-names&gt; can be used:\n<br>\nUser-Agent</p>\n\n<p style=\"margin-left:15%;\">The value of this field is the\nname of the robot the record is describing access policy\nfor. If more than one <i>User-Agent</i> field is present the\nrecord describes an identical access policy for more than\none robot. At least one field needs to be present per\nrecord. If the value is &rsquo;*&rsquo;, the record\ndescribes the default access policy for any robot that has\nnot not matched any of the other records.</p>\n\n<p style=\"margin-left:15%; margin-top: 1em\">The\n<i>User-Agent</i> fields must occur before the\n<i>Disallow</i> fields. If a record contains a\n<i>User-Agent</i> field after a <i>Disallow</i> field, that\nconstitutes a malformed record. This parser will assume that\na blank line should have been placed before that\n<i>User-Agent</i> field, and will break the record into two.\nAll the fields before the <i>User-Agent</i> field will\nconstitute a record, and the <i>User-Agent</i> field will be\nthe first field in a new record.</p>\n\n<p style=\"margin-left:11%;\">Disallow</p>\n\n<p style=\"margin-left:15%;\">The value of this field\nspecifies a partial <small>URL</small> that is not to be\nvisited. This can be a full path, or a partial path; any\n<small>URL</small> that starts with this value will not be\nretrieved</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">Unrecognized\nrecords are ignored.</p>\n\n<h2>ROBOTS.TXT EXAMPLES\n<a name=\"ROBOTS.TXT EXAMPLES\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">The following\nexample &quot;/robots.txt&quot; file specifies that no\nrobots should visit any <small>URL</small> starting with\n&quot;/cyberworld/map/&quot; or &quot;/tmp/&quot;:</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">User-agent: *\n<br>\nDisallow: /cyberworld/map/ # This is an infinite virtual URL\nspace <br>\nDisallow: /tmp/ # these will soon disappear</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">This example\n&quot;/robots.txt&quot; file specifies that no robots should\nvisit any <small>URL</small> starting with\n&quot;/cyberworld/map/&quot;, except the robot called\n&quot;cybermapper&quot;:</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">User-agent: *\n<br>\nDisallow: /cyberworld/map/ # This is an infinite virtual URL\nspace <br>\n# Cybermapper knows where to go. <br>\nUser-agent: cybermapper <br>\nDisallow:</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">This example\nindicates that no robots should visit this site further:</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\"># go away <br>\nUser-agent: * <br>\nDisallow: /</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">This is an\nexample of a malformed robots.txt file.</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\"># robots.txt\nfor ancientcastle.example.com <br>\n# I've locked myself away. <br>\nUser-agent: * <br>\nDisallow: / <br>\n# The castle is your home now, so you can go anywhere you\nlike. <br>\nUser-agent: Belle <br>\nDisallow: /west-wing/ # except the west wing! <br>\n# It's good to be the Prince... <br>\nUser-agent: Beast <br>\nDisallow:</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">This file is\nmissing the required blank lines between records. However,\nthe intention is clear.</p>\n\n<h2>SEE ALSO\n<a name=\"SEE ALSO\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">LWP::RobotUA,\nWWW::RobotRules::AnyDBM_File</p>\n\n<h2>COPYRIGHT\n<a name=\"COPYRIGHT\"></a>\n</h2>\n\n\n<p style=\"margin-left:11%; margin-top: 1em\">Copyright\n1995-2009, Gisle Aas <br>\nCopyright 1995, Martijn Koster</p>\n\n<p style=\"margin-left:11%; margin-top: 1em\">This library is\nfree software; you can redistribute it and/or modify it\nunder the same terms as Perl itself.</p>\n<hr>\n","headings":["<a href=\"#NAME\">NAME</a>","<a href=\"#SYNOPSIS\">SYNOPSIS</a>","<a href=\"#DESCRIPTION\">DESCRIPTION</a>","<a href=\"#ROBOTS.TXT\">ROBOTS.TXT</a>","<a href=\"#ROBOTS.TXT EXAMPLES\">ROBOTS.TXT EXAMPLES</a>","<a href=\"#SEE ALSO\">SEE ALSO</a>","<a href=\"#COPYRIGHT\">COPYRIGHT</a>"],"level":3}